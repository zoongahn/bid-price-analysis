import ssl
import time
from itertools import count
from typing import Any

import requests
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from pymongo.errors import DuplicateKeyError

from common.logger import setup_loggers
from common.utils import *
from common.init_mongodb import *


# SSLContextAdapter (TLS 1.2 ì´í•˜ ê°•ì œ & ë³´ì•ˆë ˆë²¨ ë‚®ì¶”ê¸°) ----------------
class SSLContextAdapter(HTTPAdapter):
	def __init__(self, ssl_context=None, **kwargs):
		self._ssl_context = ssl_context
		super().__init__(**kwargs)

	def init_poolmanager(self, connections, maxsize, block=False, **kwargs):
		if self._ssl_context is not None:
			kwargs["ssl_context"] = self._ssl_context
		self.poolmanager = PoolManager(
			num_pools=connections, maxsize=maxsize, block=block, **kwargs
		)


class DataCollector:
	def __init__(self, service_name: str | None = None, operation_number: str | int | None = None):

		self.service_name, self.operation_number = service_name, operation_number

		if service_name is None and operation_number is None:
			self.service_name, self.operation_number = input_handler()

		self.server, self.client = init_mongodb()

		self.API_BASE_DOMAIN = os.getenv("API_BASE_DOMAIN")
		self.API_SERVICE_KEY = os.getenv("API_SERVICE_KEY")

		# 1) SSLContext ìƒì„±
		ssl_ctx = ssl.create_default_context()
		# 2) TLS 1.3 ë¹„í™œì„± â†’ TLS 1.2 ì´í•˜
		ssl_ctx.maximum_version = ssl.TLSVersion.TLSv1_2
		# 3) "ë³´ì•ˆ ë ˆë²¨"ì„ 1ë¡œ ë‚®ì¶”ì–´, êµ¬ë²„ì „ Cipherê¹Œì§€ í—ˆìš©
		ssl_ctx.set_ciphers("DEFAULT:@SECLEVEL=1")

		self.session = requests.Session()
		self.session.mount("https://", SSLContextAdapter(ssl_context=ssl_ctx))

		service_info = get_service_info(service_name=self.service_name, operation_number=self.operation_number)

		service_endpoint = service_info["service_endpoint"]
		operation_info = service_info["filtered_operations"][0]
		operation_endpoint = operation_info["ì˜¤í¼ë ˆì´ì…˜ëª…(ì˜ë¬¸)"]
		self.collection_name = operation_info["raw_data_collection_name"]

		self.url = f"{self.API_BASE_DOMAIN}/{service_endpoint}/{operation_endpoint}"

		self.db = self.client.get_database("gfcon_raw")

		self.collection = self.db[self.collection_name]

		self.loggers = setup_loggers()

		self.unique_fields = operation_info["unique_fields"]

		self.params_list = {
			"notice": {
				'serviceKey': self.API_SERVICE_KEY,
				'pageNo': 1,
				'numOfRows': 100,
				'inqryDiv': 1,
				'type': 'json',
				'inqryBgnDt': None,
				'inqryEndDt': None
			},
			"bid": {
				'serviceKey': self.API_SERVICE_KEY,
				'pageNo': 1,
				'numOfRows': 100,
				'type': 'json',
				'bidNtceNo': None,
			},
			"pubData": {
				1: {
					'serviceKey': self.API_SERVICE_KEY,
					'pageNo': 1,
					'numOfRows': 100,
					'type': 'json',
					'bsnsDivCd': None,
					'bidNtceBgnDt': None,
					'bidNtceEndDt': None,
				},
				2: {
					'serviceKey': self.API_SERVICE_KEY,
					'pageNo': 1,
					'numOfRows': 500,
					'type': 'json',
					'bsnsDivCd': 3,
					'opengBgnDt': None,
					'opengEndDt': None,
				},
				3: {
					'serviceKey': self.API_SERVICE_KEY,
					'pageNo': 1,
					'numOfRows': 100,
					'type': 'json',
					'cntrctCnclsBgnDate': None,
					'cntrctCnclsEndDate': None,
				}
			}
		}

		self.date_field_map = {
			"notice": ["inqryBgnDt", "inqryEndDt"],
			"bid": [],  # ë‚ ì§œí•„ë“œ ì—†ìŒ
			"pubData": {
				1: ["bidNtceBgnDt", "bidNtceEndDt"],
				2: ["opengBgnDt", "opengEndDt"],
				3: ["cntrctCnclsBgnDate", "cntrctCnclsEndDate"]
			}
		}

	def set_date_params(self, api_type: str, params: dict, sub_type: int = None, date: str = ""):
		start = date + "0000"
		end = date + "2359"

		if api_type == "pubData":
			fields = self.date_field_map[api_type][sub_type]
		else:
			fields = self.date_field_map[api_type]

		for i, key in enumerate(fields):
			params[key] = start if i == 0 else end

		return params

	def record_txt(self, record_str: str, txt_file_name):
		"""
		ìƒˆë¡œ ì²˜ë¦¬í•œ ë‚ ì§œë¥¼ íŒŒì¼ì— í•œ ì¤„ì”© ê¸°ë¡
		"""
		root_dir_path = os.path.join(get_project_root(), "fetch_record")
		os.makedirs(root_dir_path, exist_ok=True)

		collection_name_dir_path = os.path.join(root_dir_path, self.collection_name)
		os.makedirs(collection_name_dir_path, exist_ok=True)

		file_path = os.path.join(collection_name_dir_path, txt_file_name)
		with open(file_path, "a", encoding="utf-8") as f:
			f.write(record_str + "\n")

	def get_json_with_retry(self, url, params, date, retry_interval=10):
		from requests.exceptions import JSONDecodeError, ConnectionError

		def error_handler(e: str, interval: int):
			self.loggers["application"].error(
				f'{date} - {e} ë°œìƒ, {interval}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘...')
			self.loggers["error"].error(
				f'{date} - {e} ë°œìƒ, {interval}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘...')
			time.sleep(interval)

		while True:
			try:
				response = self.session.get(url, params=params)
				try:
					return response.json()
				except JSONDecodeError:
					error_handler("JSONDecodeError", interval=retry_interval)
				except ConnectionError:
					error_handler("ConnectionError", interval=retry_interval)
			except Exception as e:
				self.loggers["application"].error(
					f"{self.collection_name} - ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
				self.loggers["error"].error(
					f"{self.collection_name} - ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
				raise

	# ê³µê³  ë°ì´í„° ë° ê¸°ì—… ë°ì´í„° ìˆ˜ì§‘ì— ì‚¬ìš©
	def collect_data_by_day(self, date: str, collect_bids: bool = False,
	                        bid_counter_by_date: dict | None = None) -> int | None | Any:

		if self.service_name == "ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤":
			api_type = "pubData"
			sub_type = self.operation_number
			params = self.params_list[api_type][sub_type].copy()
		else:
			api_type = "notice"
			sub_type = None
			params = self.params_list[api_type].copy()

		# ë‚ ì§œ í•„ë“œ ìë™ ì„¤ì •
		params = self.set_date_params(api_type, params, sub_type=sub_type, date=date)

		try:
			data = self.get_json_with_retry(self.url, params)

			total_count = data['response']['body']['totalCount']
			num_of_rows = params['numOfRows']
			total_pages = -(-total_count // num_of_rows)

			self.loggers["application"].day(f'{self.collection_name} - {date} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')
			self.loggers["day"].day(f'{self.collection_name} - {date} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')

			if collect_bids:
				try:
					db_date_count = bid_counter_by_date[f"{date[:4]}-{date[4:6]}-{date[6:]}"]
				except KeyError:
					db_date_count = 0

				# Â±5%ê¹Œì§€ í—ˆìš©í•˜ë„ë¡...
				margin_rate = 0.05
				if abs(db_date_count - total_count) <= total_count * margin_rate:
					self.loggers["application"].verify(
						f'{date} - ë°ì´í„° ê°œìˆ˜ ì°¨ì´ 5% ë‚´ì™¸ - API:{total_count} | DB:{db_date_count} PASSED')
					return None
				else:
					self.loggers["application"].verify(
						f'{date} - ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜ - API:{total_count} | DB:{db_date_count} - CONTINUE')

			total_success = 0
			total_insert = 0
			total_update = 0
			total_failed = 0

			for page in range(1, total_pages + 1):
				page_insert_count = 0
				page_update_count = 0

				params['pageNo'] = page
				data = self.get_json_with_retry(self.url, params, date)

				if 'response' in data:
					items = data['response']['body']['items']
					if isinstance(items, dict):
						items = [items]

					success_count = 0
					for item in items:
						try:
							item['collected_at'] = datetime.now()

							# ë¨¼ì € insertë¥¼ ì‹œë„, ì¤‘ë³µë˜ë©´ update ìˆ˜í–‰
							try:
								self.collection.insert_one(item)  # ìƒˆë¡œìš´ ë°ì´í„° ì‚½ì…
								page_insert_count += 1
							except DuplicateKeyError:
								# ì¤‘ë³µëœ ê²½ìš° update ìˆ˜í–‰
								item.pop("_id", None)

								update_query = {}
								for uf in self.unique_fields:
									update_query[uf] = item[uf]

								self.collection.update_one(
									update_query,
									{"$set": item}
								)
								page_update_count += 1

						except Exception as e:
							self.loggers["application"].error(
								f'ì €ì¥ ì‹¤íŒ¨: {item["bidNtceNo"]} - {item["bidNtceOrd"]}, ì—ëŸ¬: {e}')
							self.loggers["error"].error(f'ì €ì¥ ì‹¤íŒ¨: {item["bidNtceNo"]} - {item["bidNtceOrd"]}, ì—ëŸ¬: {e}')
							total_failed += 1

					success_count = page_insert_count + page_update_count
					total_insert += page_insert_count
					total_update += page_update_count
					total_success += success_count
					self.loggers['application'].fetch(
						f"{date} - {page}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ì™„ë£Œ: {success_count}({page_insert_count}+{page_update_count})ê±´")

			self.loggers["application"].day(
				f"{self.collection_name} - {date} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			self.loggers["day"].day(
				f"{self.collection_name} - {date} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			return total_success


		except Exception as e:
			self.loggers["application"].error(f"{self.collection_name} - {date} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
			self.loggers["error"].error(f"{self.collection_name} - {date} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
			raise

	def collect_all_data_by_day(self, start_date: str, end_date: str):
		# íˆ¬ì°° ë°ì´í„° ìˆ˜ì§‘ì¸ì§€?
		collect_bids: bool = (self.collection_name == "ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤.ë°ì´í„°ì…‹ê°œë°©í‘œì¤€ì—ë”°ë¥¸ë‚™ì°°ì •ë³´")

		# ìœ ë‹ˆí¬ ì¸ë±ìŠ¤ ì„¤ì •
		unique_fields_query: list[tuple] = []  # Like [("bidNtceNo", 1), ("bidNtceOrd", 1)]
		for uf in self.unique_fields:
			unique_fields_query.append(tuple([uf, 1]))

		# ë³µí•© ì¸ë±ìŠ¤ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œë¨)
		self.collection.create_index(unique_fields_query, unique=True)

		date_list = list(generate_dates(start_date, end_date))

		self.loggers["application"].info(f"{start_date} ~ {end_date} ë‚´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.")

		bid_counter_by_date = self.count_by_openg_date() if collect_bids else None

		pending_dates = date_list

		attempt = 1

		while pending_dates:
			# ì´ë²ˆì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ë‚ ì§œ ê¸°ë¡
			error_dates = []
			for date in pending_dates:
				try:
					result = self.collect_data_by_day(date, collect_bids, bid_counter_by_date=bid_counter_by_date)
					self.record_txt(date, "fetched_date.txt")
				except Exception as e:
					self.loggers["error"].error(f"{self.collection_name} - {date} - ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
					self.record_txt(date, "error_date.txt")
					error_dates.append(date)

			if not error_dates:
				self.loggers["application"].info("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				break

			# ì—ëŸ¬ê°€ ë°œìƒí•œ ë‚ ì§œë“¤ì— ëŒ€í•´ ë‹¤ì‹œ ì‹œë„
			self.loggers["application"].warning(f"âš ï¸ [Attempt {attempt}] {len(error_dates)}ê°œì˜ ë‚ ì§œì—ì„œ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì§„í–‰.")
			pending_dates = error_dates  # ì—ëŸ¬ ë°œìƒí•œ ë‚ ì§œë“¤ë§Œ ë‹¤ì‹œ ì‹œë„
			attempt += 1  # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì‹œë„ íšŸìˆ˜ ì¦ê°€

	def collect_bids_by_NtceNo(self, NtceNo: str):
		try:
			params = self.params_list["bid"]
			params["bidNtceNo"] = NtceNo

			response = self.session.get(self.url, params=params)

			data = response.json()

			total_count = data['response']['body']['totalCount']
			num_of_rows = params['numOfRows']
			total_pages = -(-total_count // num_of_rows)

			self.loggers["application"].day(f'{self.collection_name} - {NtceNo} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')
			self.loggers["day"].day(f'{self.collection_name} - {NtceNo} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')

			total_success = 0
			total_insert = 0
			total_update = 0
			total_failed = 0

			for page in range(1, total_pages + 1):
				page_insert_count = 0
				page_update_count = 0

				params['pageNo'] = page
				response = self.session.get(self.url, params=params)
				data = response.json()

				if 'response' in data:
					items = data['response']['body']['items']
					if isinstance(items, dict):
						items = [items]

					success_count = 0
					for item in items:
						try:
							item['collected_at'] = datetime.now()

							# ë¨¼ì € insertë¥¼ ì‹œë„, ì¤‘ë³µë˜ë©´ update ìˆ˜í–‰
							try:
								self.collection.insert_one(item)  # ìƒˆë¡œìš´ ë°ì´í„° ì‚½ì…
								page_insert_count += 1
							except DuplicateKeyError:
								# ì¤‘ë³µëœ ê²½ìš° update ìˆ˜í–‰
								item.pop("_id", None)

								update_query = {}
								for uf in self.unique_fields:
									update_query[uf] = item[uf]

								self.collection.update_one(
									update_query,
									{"$set": item}
								)
								page_update_count += 1

						except Exception as e:
							self.loggers["application"].error(
								f'ì €ì¥ ì‹¤íŒ¨: {item["bidNtceNo"]} - {item["prcbdrBizno"]}, ì—ëŸ¬: {e}')
							self.loggers["error"].error(f'ì €ì¥ ì‹¤íŒ¨: {item["bidNtceNo"]} - {item["prcbdrBizno"]}, ì—ëŸ¬: {e}')
							total_failed += 1

					success_count = page_insert_count + page_update_count
					total_insert += page_insert_count
					total_update += page_update_count
					total_success += success_count
					self.loggers['application'].fetch(
						f"{self.collection_name} - {page}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ì™„ë£Œ: {success_count}({page_insert_count}+{page_update_count})ê±´")

			self.loggers["application"].day(
				f"{self.collection_name} - {NtceNo} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			self.loggers["day"].day(
				f"{self.collection_name} - {NtceNo} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			return total_success

		except Exception as e:
			self.loggers["application"].error(f"{self.collection_name} - {NtceNo} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
			                                  exc_info=True)
			self.loggers["error"].error(f"{self.collection_name} - {NtceNo} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
			raise

	def get_notice_number_list(self):
		db = self.client.get_database("gfcon_raw")
		collection = db.get_collection("ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤.ë‚™ì°°ëœëª©ë¡í˜„í™©ê³µì‚¬ì¡°íšŒ")
		result = collection.find({}, {"bidNtceNo": 1, "_id": 0})

		result = [doc['bidNtceNo'] for doc in result]

		return result

	def collect_all_bids_by_NtceNo(self):

		unique_fields_query: list[tuple] = []  # Like [("bidNtceNo", 1), ("bidNtceOrd", 1)]
		for uf in self.unique_fields:
			unique_fields_query.append(tuple([uf, 1]))

		# ë³µí•© ì¸ë±ìŠ¤ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œë¨)
		self.collection.create_index(unique_fields_query, unique=True)

		collection_notices = self.db.get_collection("ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤.ë‚™ì°°ëœëª©ë¡í˜„í™©ê³µì‚¬ì¡°íšŒ")

		# 1. bids_info_is_collected=Falseì¸ ê³µê³ ë²ˆí˜¸ë§Œ ê°€ì ¸ì˜¤ê¸°
		notice_number_list = [
			doc["bidNtceNo"]
			for doc in collection_notices.find(
				{"bids_info_is_collected": False},
				{"bidNtceNo": 1, "_id": 0}
			)
		]

		if not notice_number_list:
			self.loggers["application"].info("âœ… ìˆ˜ì§‘í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
			return

		self.loggers["application"].info(f"{notice_number_list[0]} ~ {notice_number_list[-1]} ë‚´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.")

		pending_notices = notice_number_list
		attempt = 1

		while pending_notices:
			# ì´ë²ˆì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ê³µê³ ë²ˆí˜¸ ê¸°ë¡
			error_notices = []
			for notice_number in pending_notices:
				try:
					result = self.collect_bids_by_NtceNo(notice_number)
					self.record_txt(notice_number, "fetched_notice.txt")

					# ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ í•´ë‹¹ ê³µê³ ì˜ bids_info_is_collectedë¥¼ Trueë¡œ ì—…ë°ì´íŠ¸
					collection_notices.update_one(
						{"bidNtceNo": notice_number},
						{"$set": {"bids_info_is_collected": True}}
					)

				except Exception as e:
					self.loggers["error"].error(f"{self.collection_name} - {notice_number} - ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
					self.record_txt(notice_number, "error_notice.txt")
					error_notices.append(notice_number)

			if not error_notices:
				self.loggers["application"].info("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				break

			# ì—ëŸ¬ê°€ ë°œìƒí•œ ë‚ ì§œë“¤ì— ëŒ€í•´ ë‹¤ì‹œ ì‹œë„
			self.loggers["application"].warning(f"âš ï¸ [{attempt}ì°¨ ì‹œë„] {len(error_notices)}ê°œì˜ ë‚ ì§œì—ì„œ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì§„í–‰.")
			pending_notices = error_notices  # ì—ëŸ¬ ë°œìƒí•œ ë‚ ì§œë“¤ë§Œ ë‹¤ì‹œ ì‹œë„
			attempt += 1  # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì‹œë„ íšŸìˆ˜ ì¦ê°€

	def count_by_openg_date(self) -> dict:
		pipeline = [
			{
				"$group": {
					"_id": "$opengDate",
					"count": {"$sum": 1}
				}
			},
			{
				"$sort": {"_id": 1}
			}
		]

		result = list(self.collection.aggregate(pipeline))
		return {item['_id']: item['count'] for item in result}

	def execute(self):
		if self.service_name == "ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤" and self.operation_number == 13:
			self.collect_all_bids_by_NtceNo()
		else:
			start_date = '2010-01-04'
			end_date = '2010-01-04'
			self.collect_all_data_by_day(start_date, end_date)
