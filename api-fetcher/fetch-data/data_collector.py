import ssl
import time
import math
from itertools import count
from typing import Any, Dict, List, Optional

import requests
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from pymongo.errors import DuplicateKeyError

from common.logger import setup_loggers
from common.utils import *
from common.init_mongodb import *


# SSLContextAdapter (TLS 1.2 ì´í•˜ ê°•ì œ & ë³´ì•ˆë ˆë²¨ ë‚®ì¶”ê¸°) ----------------
class SSLContextAdapter(HTTPAdapter):
	def __init__(self, ssl_context=None, **kwargs):
		self._ssl_context = ssl_context
		super().__init__(**kwargs)

	def init_poolmanager(self, connections, maxsize, block=False, **kwargs):
		if self._ssl_context is not None:
			kwargs["ssl_context"] = self._ssl_context
		self.poolmanager = PoolManager(
			num_pools=connections, maxsize=maxsize, block=block, **kwargs
		)


class ApiClient:
	"""Handles all HTTP communication with retry logic."""

	def __init__(self, base_url: str):
		self.base_url = base_url
		self.logger = setup_loggers()
		self.session = self._create_ssl_session()

	# ---------------------------------------------------------------------
	# Public helpers
	# ---------------------------------------------------------------------
	def get(self, endpoint: str, params: Dict[str, Any], retry_interval: int = 10) -> Dict[str, Any]:
		"""GET with automatic JSON decode & retry."""
		full_url = f"{self.base_url}/{endpoint}"
		while True:
			try:
				response = self.session.get(full_url, params=params, timeout=30)
				response.raise_for_status()
				return response.json()
			except (requests.exceptions.ConnectionError, requests.exceptions.JSONDecodeError) as exc:
				self.logger["application"].error(
					f"{exc.__class__.__name__} while requesting {full_url} â€“ retry in {retry_interval}s"
				)
				time.sleep(retry_interval)
				continue
			except Exception as exc:  # pragma: no cover
				self.logger["error"].error("Unhandled exception in ApiClient", exc_info=True)
				raise

	# ------------------------------------------------------------------
	# Private helpers
	# ------------------------------------------------------------------
	@staticmethod
	def _create_ssl_session() -> requests.Session:
		# 1) SSLContext ìƒì„±
		ssl_ctx = ssl.create_default_context()
		# 2) TLS 1.3 ë¹„í™œì„± â†’ TLS 1.2 ì´í•˜
		ssl_ctx.maximum_version = ssl.TLSVersion.TLSv1_2
		# 3) "ë³´ì•ˆ ë ˆë²¨"ì„ 1ë¡œ ë‚®ì¶”ì–´, êµ¬ë²„ì „ Cipherê¹Œì§€ í—ˆìš©
		ssl_ctx.set_ciphers("DEFAULT:@SECLEVEL=1")
		session = requests.Session()
		session.mount("https://", SSLContextAdapter(ssl_context=ssl_ctx))
		return session


# ---------------------------------------------------------------------------
# Persistence layer
# ---------------------------------------------------------------------------
class MongoWriter:
	"""Wraps MongoDB collection with safe insert/update (upsert) behaviour."""

	def __init__(self, db, collection_name: str, unique_fields: List[str]):
		self.collection = db[collection_name]
		self.unique_fields = unique_fields
		self._ensure_index()

	def upsert(self, item: Dict[str, Any]) -> str:
		"""
			"insert"  -> ìƒˆ ë¬¸ì„œê°€ ì‚½ì…ë¨
			"update"  -> ì¤‘ë³µìœ¼ë¡œ ì¸í•´ ìˆ˜ì • ìˆ˜í–‰
			"error"   -> ë‹¤ë¥¸ ì˜ˆì™¸ê°€ ë°œìƒ (ë¡œê·¸ëŠ” ì—¬ê¸°ì„œ ë‚¨ê¸°ê³ , ì˜ˆì™¸ëŠ” ìƒìœ„ì—ì„œ ì²˜ë¦¬)
		"""
		try:
			# 1) insert ì‹œë„
			self.collection.insert_one(item)
			return "insert"

		except DuplicateKeyError:
			# 2) ì¤‘ë³µì´ë©´ update ìˆ˜í–‰
			item.pop("_id", None)  # _id í•„ë“œ ì œê±° (ì›ë³¸ ë¡œì§ê³¼ ë™ì¼)

			update_query = {field: item[field] for field in self.unique_fields}
			self.collection.update_one(update_query, {"$set": item})
			return "update"

		except Exception as exc:
			# 3) ê¸°íƒ€ ì—ëŸ¬ëŠ” ë‚´ë¶€ ë¡œê¹… í›„ 'error' ë°˜í™˜
			from common.logger import setup_loggers

			loggers = setup_loggers()
			loggers["application"].error(f"Mongo upsert ì‹¤íŒ¨: {exc}", exc_info=True)
			loggers["error"].error(f"Mongo upsert ì‹¤íŒ¨: {exc}", exc_info=True)
			return "error"

	def _ensure_index(self) -> None:
		index_spec = [(field, 1) for field in self.unique_fields]
		self.collection.create_index(index_spec, unique=True)


# ---------------------------------------------------------------------------
# Parameter builder
# ---------------------------------------------------------------------------
class ParamsBuilder:
	"""Generate API params & map date fields."""

	def __init__(self, api_service_key: str, num_of_rows: int = 500):
		self.api_service_key = api_service_key
		self.num_of_rows = num_of_rows
		self.params_list = self._build_params_list()
		self.date_field_map = self._build_date_field_map()

	def build(self, api_type: str, date: str, sub_type: Optional[int] = None) -> Dict[str, Any]:
		params = (
			self.params_list[api_type][sub_type].copy()
			if api_type == "pubData"
			else self.params_list[api_type].copy()
		)
		return self._set_date_params(api_type, params, date, sub_type)

	# ------------------------------------------------------------------
	# Internal helpers
	# ------------------------------------------------------------------
	def _set_date_params(self, api_type: str, params: Dict[str, Any], date: str, sub_type: Optional[int]):
		start, end = f"{date}0000", f"{date}2359"
		fields = (
			self.date_field_map[api_type][sub_type]
			if api_type == "pubData"
			else self.date_field_map[api_type]
		)
		for idx, key in enumerate(fields):
			params[key] = start if idx == 0 else end
		return params

	def _build_params_list(self):
		sk, n = self.api_service_key, self.num_of_rows
		return {
			"notice": {
				"serviceKey": sk,
				"pageNo": 1,
				"numOfRows": n,
				"inqryDiv": 1,
				"type": "json",
				"inqryBgnDt": None,
				"inqryEndDt": None,
			},
			"bid": {
				"serviceKey": sk,
				"pageNo": 1,
				"numOfRows": n,
				"type": "json",
				"bidNtceNo": None,
			},
			"pubData": {
				1: {
					"serviceKey": sk,
					"pageNo": 1,
					"numOfRows": n,
					"type": "json",
					"bsnsDivCd": None,
					"bidNtceBgnDt": None,
					"bidNtceEndDt": None,
				},
				2: {
					"serviceKey": sk,
					"pageNo": 1,
					"numOfRows": n,
					"type": "json",
					"bsnsDivCd": 3,
					"opengBgnDt": None,
					"opengEndDt": None,
				},
				3: {
					"serviceKey": sk,
					"pageNo": 1,
					"numOfRows": n,
					"type": "json",
					"cntrctCnclsBgnDate": None,
					"cntrctCnclsEndDate": None,
				},
			},
		}

	@staticmethod
	def _build_date_field_map():
		return {
			"notice": ["inqryBgnDt", "inqryEndDt"],
			"bid": [],
			"pubData": {
				1: ["bidNtceBgnDt", "bidNtceEndDt"],
				2: ["opengBgnDt", "opengEndDt"],
				3: ["cntrctCnclsBgnDate", "cntrctCnclsEndDate"],
			},
		}


# ---------------------------------------------------------------------------
# Record writer
# ---------------------------------------------------------------------------
class RecordWriter:
	"""Append processed ids/dates to text files under fetch_record/"""

	def __init__(self, collection_name: str):
		self.root = os.path.join(get_project_root(), "fetch_record", collection_name)
		os.makedirs(self.root, exist_ok=True)

	def append(self, text: str, filename: str):
		with open(os.path.join(self.root, filename), "a", encoding="utf-8") as fh:
			fh.write(text + "\n")


# ---------------------------------------------------------------------------
# DataCollector orchestrator
# ---------------------------------------------------------------------------
class DataCollector:
	"""Topâ€‘level orchestrator that glues API â†” DB â†” filesystem."""

	def __init__(
			self,
			service_name: str | None = None,
			operation_number: str | int | None = None,
			year: str | int | None = None,
	) -> None:
		# ------------------------------------------------------------------
		# Resolve user input / defaults
		# ------------------------------------------------------------------
		self.executed_year = year
		if service_name is None and operation_number is None:
			service_name, operation_number = input_handler()
		self.service_name, self.operation_number = service_name, operation_number

		# ------------------------------------------------------------------
		# External resources
		# ------------------------------------------------------------------
		self.server, self.client = init_mongodb()
		self.db = self.client.get_database("gfcon_raw")

		self.API_BASE_DOMAIN = os.getenv("API_BASE_DOMAIN", "https://api.g2b.go.kr")
		self.API_SERVICE_KEY = os.getenv("API_SERVICE_KEY", "")

		# ------------------------------------------------------------------
		# Service metadata (dynamic per user choice)
		# ------------------------------------------------------------------
		svc_info = get_service_info(service_name=self.service_name, operation_number=self.operation_number)
		op_info = svc_info["filtered_operations"][0]

		self.collection_name = op_info["raw_data_collection_name"]
		service_endpoint = svc_info["service_endpoint"]
		operation_endpoint = op_info["ì˜¤í¼ë ˆì´ì…˜ëª…(ì˜ë¬¸)"]
		self.endpoint = f"{service_endpoint}/{operation_endpoint}"
		self.unique_fields = op_info["unique_fields"]

		# ------------------------------------------------------------------
		# Utilities / helpers
		# ------------------------------------------------------------------
		self.loggers = setup_loggers(year=self.executed_year)
		self.api = ApiClient(self.API_BASE_DOMAIN, self.loggers)
		self.params_builder = ParamsBuilder(self.API_SERVICE_KEY)
		self.mongo = MongoWriter(self.db, self.collection_name, self.unique_fields)
		self.recorder = RecordWriter(self.collection_name)

	# ê³µê³  ë°ì´í„° ë° ê¸°ì—… ë°ì´í„° ìˆ˜ì§‘ì— ì‚¬ìš©
	def collect_data_by_day(self, date: str, collect_bids: bool = False,
	                        bid_counter_by_date: dict | None = None) -> int | None | Any:

		if self.service_name == "ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤":
			api_type, sub_type = "pubData", self.operation_number
		else:
			api_type, sub_type = "notice", None
		params = self.params_builder.build(api_type, date, sub_type)

		try:
			data = self.api.get(self.endpoint, params)

			total_count = data['response']['body']['totalCount']
			num_of_rows = params['numOfRows']
			total_pages = -(-total_count // num_of_rows)

			self.loggers["application"].day(f'{self.collection_name} - {date} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')
			self.loggers["day"].day(f'{self.collection_name} - {date} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}')

			if collect_bids:
				try:
					db_date_count = bid_counter_by_date[f"{date[:4]}-{date[4:6]}-{date[6:]}"]
				except KeyError:
					db_date_count = 0

				# Â±5%ê¹Œì§€ í—ˆìš©í•˜ë„ë¡...
				margin_rate = 0.05
				if abs(db_date_count - total_count) <= total_count * margin_rate:
					self.loggers["application"].verify(
						f'{date} - ë°ì´í„° ê°œìˆ˜ ì°¨ì´ 5% ë‚´ì™¸ - API:{total_count} | DB:{db_date_count} PASSED')
					return None
				else:
					self.loggers["application"].verify(
						f'{date} - ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜ - API:{total_count} | DB:{db_date_count} - CONTINUE')

			total_success, total_insert, total_update, total_failed = 0, 0, 0, 0

			for page in range(1, total_pages + 1):
				page_insert_count = 0
				page_update_count = 0

				params['pageNo'] = page
				data = self.api.get(self.endpoint, params)

				items = data['response']['body']['items']
				if isinstance(items, dict):
					items = [items]

				for item in items:
					result = self.mongo.upsert(item)
					if result == "insert":
						page_insert_count += 1
					elif result == "update":
						page_update_count += 1
					else:
						total_failed += 1

				success_count = page_insert_count + page_update_count
				total_insert += page_insert_count
				total_update += page_update_count
				total_success += success_count
				self.loggers['application'].fetch(
					f"{date} - {page}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ì™„ë£Œ: {success_count}({page_insert_count}+{page_update_count})ê±´")

			self.loggers["application"].day(
				f"{self.collection_name} - {date} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			self.loggers["day"].day(
				f"{self.collection_name} - {date} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: {total_success}({total_insert}+{total_update})")
			return total_success


		except Exception as e:
			self.loggers["application"].error(f"{self.collection_name} - {date} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
			self.loggers["error"].error(f"{self.collection_name} - {date} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
			raise

	def collect_all_data_by_day(self, start_date: str, end_date: str) -> None:
		# íˆ¬ì°° ë°ì´í„° ìˆ˜ì§‘ì¸ì§€?
		collect_bids: bool = (self.collection_name == "ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤.ë°ì´í„°ì…‹ê°œë°©í‘œì¤€ì—ë”°ë¥¸ë‚™ì°°ì •ë³´")

		date_list = list(generate_dates(start_date, end_date))
		self.loggers["application"].info(f"{start_date} ~ {end_date} ë‚´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.")

		bid_counter_by_date = self.count_data_by_date(start_date, end_date) if collect_bids else None

		pending_dates = date_list
		attempt = 1

		while pending_dates:
			# ì´ë²ˆì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ë‚ ì§œ ê¸°ë¡
			error_dates: list[str] = []

			for date in pending_dates:
				try:
					self.collect_data_by_day(date, collect_bids, bid_counter_by_date=bid_counter_by_date)
				except Exception as e:
					self.loggers["error"].error(f"{self.collection_name} - {date} - ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
					self.recorder.append(date, "error_date.txt")
					error_dates.append(date)

			if not error_dates:
				self.loggers["application"].info("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				break

			# ì—ëŸ¬ê°€ ë°œìƒí•œ ë‚ ì§œë“¤ì— ëŒ€í•´ ë‹¤ì‹œ ì‹œë„
			self.loggers["application"].warning(f"âš ï¸ [Attempt {attempt}] {len(error_dates)}ê°œì˜ ë‚ ì§œì—ì„œ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì§„í–‰.")
			pending_dates = error_dates  # ì—ëŸ¬ ë°œìƒí•œ ë‚ ì§œë“¤ë§Œ ë‹¤ì‹œ ì‹œë„
			attempt += 1  # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì‹œë„ íšŸìˆ˜ ì¦ê°€

	def collect_bids_by_NtceNo(self, NtceNo: str):
		try:
			# 1) íŒŒë¼ë¯¸í„° ì¤€ë¹„ (ParamsBuilder ë‚´ë¶€ ê¸°ë³¸ê°’ í™œìš©)
			params = self.params_builder.params_list["bid"].copy()
			params["bidNtceNo"] = NtceNo

			# 2) ì²« í˜ì´ì§€ í˜¸ì¶œ â†’ ì „ì²´ ê±´ìˆ˜ íŒŒì•…
			data = self.api.get(self.endpoint, params)
			total_count = data["response"]["body"]["totalCount"]
			num_rows = params["numOfRows"]
			total_pages = -(-total_count // num_rows)  # ceiling division

			self.loggers["application"].day(f"{self.collection_name} - {NtceNo} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}")
			self.loggers["day"].day(f"{self.collection_name} - {NtceNo} - ì „ì²´ ë°ì´í„° ìˆ˜: {total_count}")

			total_success = total_insert = total_update = total_failed = 0

			# 3) í˜ì´ì§€ ë£¨í”„
			for page in range(1, total_pages + 1):
				page_insert_count = page_update_count = 0

				params["pageNo"] = page
				data = self.api.get(self.endpoint, params)

				items = data["response"]["body"]["items"]
				if isinstance(items, dict):
					items = [items]

				for item in items:
					item["collected_at"] = datetime.now()
					result = self.mongo.upsert(item)
					if result == "insert":
						page_insert_count += 1
					elif result == "update":
						page_update_count += 1
					else:
						total_failed += 1

				success_count = page_insert_count + page_update_count
				total_insert += page_insert_count
				total_update += page_update_count
				total_success += success_count

				self.loggers["application"].fetch(
					f"{self.collection_name} - {page}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ì™„ë£Œ: "
					f"{success_count}({page_insert_count}+{page_update_count})ê±´"
				)

			# 4) ìµœì¢… ìš”ì•½ ë¡œê·¸
			self.loggers["application"].day(
				f"{self.collection_name} - {NtceNo} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: "
				f"{total_success}({total_insert}+{total_update})"
			)
			self.loggers["day"].day(
				f"{self.collection_name} - {NtceNo} - ìµœì¢… ì €ì¥ ê±´ìˆ˜: "
				f"{total_success}({total_insert}+{total_update})"
			)
			return total_success

		except Exception as e:
			self.loggers["application"].error(
				f"{self.collection_name} - {NtceNo} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True
			)
			self.loggers["error"].error(
				f"{self.collection_name} - {NtceNo} - ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True
			)
			raise

	def get_notice_number_list(self):
		collection = self.db.get_collection("ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤.ë‚™ì°°ëœëª©ë¡í˜„í™©ê³µì‚¬ì¡°íšŒ")
		result = collection.find({}, {"bidNtceNo": 1, "_id": 0})

		result = [doc['bidNtceNo'] for doc in result]

		return result

	def collect_all_bids_by_NtceNo(self):

		collection_notices = self.db.get_collection("ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤.ë‚™ì°°ëœëª©ë¡í˜„í™©ê³µì‚¬ì¡°íšŒ")

		# 1. bids_info_is_collected=Falseì¸ ê³µê³ ë²ˆí˜¸ë§Œ ê°€ì ¸ì˜¤ê¸°
		notice_number_list = [
			doc["bidNtceNo"]
			for doc in collection_notices.find(
				{"bids_info_is_collected": False},
				{"bidNtceNo": 1, "_id": 0}
			)
		]

		if not notice_number_list:
			self.loggers["application"].info("âœ… ìˆ˜ì§‘í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
			return

		self.loggers["application"].info(f"{notice_number_list[0]} ~ {notice_number_list[-1]} ë‚´ ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.")

		pending_notices = notice_number_list
		attempt = 1

		while pending_notices:
			# ì´ë²ˆì‹œë„ì—ì„œ ì‹¤íŒ¨í•œ ê³µê³ ë²ˆí˜¸ ê¸°ë¡
			error_notices = []
			for notice_number in pending_notices:
				try:
					result = self.collect_bids_by_NtceNo(notice_number)
					self.recorder.append(notice_number, "fetched_notice.txt")

					# ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ í•´ë‹¹ ê³µê³ ì˜ bids_info_is_collectedë¥¼ Trueë¡œ ì—…ë°ì´íŠ¸
					collection_notices.update_one(
						{"bidNtceNo": notice_number},
						{"$set": {"bids_info_is_collected": True}}
					)

				except Exception as e:
					self.loggers["error"].error(f"{self.collection_name} - {notice_number} - ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
					self.recorder.append(notice_number, "error_notice.txt")
					error_notices.append(notice_number)

			if not error_notices:
				self.loggers["application"].info("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				break

			# ì—ëŸ¬ê°€ ë°œìƒí•œ ë‚ ì§œë“¤ì— ëŒ€í•´ ë‹¤ì‹œ ì‹œë„
			self.loggers["application"].warning(f"âš ï¸ [{attempt}ì°¨ ì‹œë„] {len(error_notices)}ê°œì˜ ë‚ ì§œì—ì„œ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì§„í–‰.")
			pending_notices = error_notices  # ì—ëŸ¬ ë°œìƒí•œ ë‚ ì§œë“¤ë§Œ ë‹¤ì‹œ ì‹œë„
			attempt += 1  # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì‹œë„ íšŸìˆ˜ ì¦ê°€

	def count_data_by_date(self, start_date: str, end_date: str) -> dict:
		pipeline = []

		# íˆ¬ì°°ë°ì´í„° ìˆ˜ì§‘ì˜ ê²½ìš°
		if self.collection_name == "ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤.ë°ì´í„°ì…‹ê°œë°©í‘œì¤€ì—ë”°ë¥¸ë‚™ì°°ì •ë³´":
			date_field_name = "opengDate"

			pipeline = [
				{
					"$match": {
						date_field_name: {
							"$gte": start_date,
							"$lte": end_date
						}
					}
				},
				{
					"$group": {
						"_id": f"${date_field_name}",
						"count": {"$sum": 1}
					}
				},
				{
					"$sort": {"_id": 1}
				}
			]

		# ê³µê³  ê¸°ì´ˆê¸ˆì•¡ ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘ì˜ ê²½ìš°
		elif self.collection_name == "ì…ì°°ê³µê³ ì •ë³´ì„œë¹„ìŠ¤.ì…ì°°ê³µê³ ëª©ë¡ì •ë³´ì—ëŒ€í•œê³µì‚¬ê¸°ì´ˆê¸ˆì•¡ì¡°íšŒ":
			date_field_name = "bssamtOpenDt"
		elif self.collection_name == "ì…ì°°ê³µê³ ì •ë³´ì„œë¹„ìŠ¤.ì…ì°°ê³µê³ ëª©ë¡ì •ë³´ì—ëŒ€í•œê³µì‚¬ì¡°íšŒ":
			date_field_name = "rgstDt"

		result = list(self.mongo.collection.aggregate(pipeline))
		return {item['_id']: item['count'] for item in result}

	def execute(self):
		# íŠ¹ì •ë…„ë„ ìˆ˜ì§‘
		if self.executed_year:
			start_date = f'{self.executed_year}-01-01'
			end_date = f'{self.executed_year}-12-31'
			self.collect_all_data_by_day(start_date, end_date)

		# ì „ì²´ìˆ˜ì§‘
		else:
			if self.service_name == "ë‚™ì°°ì •ë³´ì„œë¹„ìŠ¤" and self.operation_number == 13:
				self.collect_all_bids_by_NtceNo()
			else:
				start_date = '2010-01-01'
				end_date = '2024-12-31'
				self.collect_all_data_by_day(start_date, end_date)
